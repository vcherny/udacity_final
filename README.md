# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
A bank marketing dataset containing information about some client information like age, occupancy, education, martial status etc, is given. 
Additionally, there are macroeconomical variables such as consumer price index, consumer confidence index, as well as interest rate (euribor).
We seek to predict a variable called "y" (also part of the dataframe) based on all the other variables. It is not exactly clear what the meaning
of the dependent variable is, but a first guess would be that it shows if the client would be interested in a loan.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model is VotingEnsamble from AutoML. This was concluded by comparing the rmse metrics for the best models from the two setups.
## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The data itself is described above. After it is downloaded from the website it is fed into the clean_date function from train.py.
A number of preprocessing steps are taken in the function, some of which are conversion of the name of the month and weekdays to numerical values.
For some variables, including the dependent one, logical values of 0 and 1 are assigned. 
The output of the function is two datasets - one containing all the independent variables, the other one - the dependent one.
The preprocessed data is used in the main function where it is further split into the training and testing dataset. 
After the logistic regression has been trained on the former, the latter is used for calculating the accuracy score which is used for the model assessment.
Accuracy is one of the metrics used to construct the hyper drive configurator (HyperDriveConfig). Obviously, maximization policy is applied for this parameter.
The other two parameters employed by the configurator are the parameter sampler and the early termination policy (described below).
**What are the benefits of the parameter sampler you chose?**
A random Sampler is chosen as a parameter sampler. The benefit of this sampler compared to the others is efficiency.
Namely, it allows one to randomly select a limited number of parameters, and the randomness ensures that a relatively large universe of parameters is covered.
Hence, compared to the alteratives, i.e. grid sampling, it does not test all the parameters hence saves time. 
Additionally, the algorithm supports the early termination policy, also adding to the efficiency, i.e. the run will be terminated if at early stage it has been detected that the accuracy is not good enough.
I chose a uniform distribution for the random sampler. This allowed the algorithm to select the potential hyperparameters with equal probability, hence covering the whole parameter space specified.
**What are the benefits of the early stopping policy you chose?**
I chose only the slack factor as the input to the termination policy. It determines the distance (in terms of accuracy) between the current and the best run.
I set the value to 0.25, which would mean that if the current run is not exceeding in accuracy the best one by 25%, it will be cancelled.
The value is a balance between efficiency and not cancelling a large number of potentially good runs due to mis-estimation of the accuracy metric at the early calculation stage.
## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The best model is called VotingEnsemble. It is a superposition of 8 combinations of a data transformer and a training algorithm:
SparseNormalzier+RandomForestRergessor; 2x StandardScalerWrapper+XGBoostRegressor; 2x SparseNormalizer+XGBoostRegressor; MaxAbsScaler+LightGBM;
MaxAbsScaler+XGBoostRegressor; MaxAbsScaler+LightGBM. The two combinations above differ in the hyperparameters, e.g. for the tree booster. 
In tern, another hyperparameter is the ensemble weights, with the largest (33%) being MaxAbsScaler+XGBoostRegressor.
R2 is 0.43530 for this model.
## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
As it was mentioned above, the best model is VotingEnsamble. With rmse of 0.2351 it performs substantially better than the logistic regression model with rmse of 0.297.
The reason from accuracy is stemming from multiple factors. Firstly, by construction logistic regression is a linear model which only accounts for linear effects.
Secondly, ML is applying data transformations (scaling, normalization) before doing the fit, which adds to accuracy and efficiency.
Thirdly, it might be that the best parameter is still not the optimim for the logistic regression model, and one might try different hyperparameter sampling in order to find a better performing model.
## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Performance-wise one can improve the regression model by simply moving the preprocessing step from the python file to the main notebook.
With the current estimator being the full python script, preprocessing is one for every run. This is redundant and slows down the calculations.
Regression model can benefit from another hyperparameter sampling - grid or Bayesian. These two methods will slower the process, however increase the chance to find the optimal parameter.
Increase maximum number of total runs (currently 100) will simply allow for more candidates models, hence, again, increase the chance of finding a better one
Changing the termination policy might also have a positive effect. Once we change the threshold such, that less models get tetminated on an earlier stage, a number of false positive errors will decrease, i.e. there will be a chance to observe the results of the models previously showing unacceptable performance on the early stage. The chance is low, howewer non-zero that some of these models on the final stage provide good results.
As to the AutoML model, the two most obvious improvements are an increase of the experiment timeout and the number of cross-validations. The former will allow for experiment to last longer, hence to explore more hyperparameters and potential models, the latter will make sure that the results are more reliable. 